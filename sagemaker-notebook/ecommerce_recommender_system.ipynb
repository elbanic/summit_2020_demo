{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Recommender System with SageMaker, MXNet, and Gluon\n",
    "_**Making Video Recommendations Using Neural Networks and Embeddings**_\n",
    "\n",
    "--- \n",
    "\n",
    "---\n",
    "\n",
    "*This work is based on content from the [Cyrus Vahid's 2017 re:Invent Talk](https://github.com/cyrusmvahid/gluontutorials/blob/master/recommendations/MLPMF.ipynb)*\n",
    "\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Setup](#Setup)\n",
    "1. [Data](#Data)\n",
    "  1. [Explore](#Explore)\n",
    "  1. [Clean](#Clean)\n",
    "  1. [Prepare](#Prepare)\n",
    "1. [Train Locally](#Train-Locally)\n",
    "  1. [Define Network](#Define-Network)\n",
    "  1. [Set Parameters](#Set-Parameters)\n",
    "  1. [Execute](#Execute)\n",
    "1. [Train with SageMaker](#Train-with-SageMaker)\n",
    "  1. [Wrap Code](#Wrap-Code)\n",
    "  1. [Move Data](#Move-Data)\n",
    "  1. [Submit](#Submit)\n",
    "1. [Host](#Host)\n",
    "  1. [Evaluate](#Evaluate)\n",
    "1. [Wrap-up](#Wrap-up)\n",
    "\n",
    "---\n",
    "\n",
    "## Background\n",
    "\n",
    "AWS Seoul Summit 2020의 '아직도 파이썬으로 머신러닝하니? 난 SQL로 바로 쓴다' 세션 데모를 위한 SageMaker 추천 모델 학습 과정을 보여주기 위한 노트북입니다. Kaggle의 [eCommerce behavior data from multi category store](https://www.kaggle.com/mkechinov/ecommerce-behavior-data-from-multi-category-store) 데이터를 학습하여 User에게 Product를 제공하는 모델을 만들고 추론 endpoint를 구성합니다. 이 노트북은 SageMaker Example Notebooks으로 제공되는 [Gluon Recommender System](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_applying_machine_learning/gluon_recommender_system/gluon_recommender_system.ipynb) 을 참고하여 작성되었습니다. 모델 훈련/평가/추론에 동일한 방식을 적용하였으니 착오 없으시길 바랍니다.\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "_This notebook was created and tested on an ml.p3.2xlarge notebook instance._\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the `get_execution_role()` call with the appropriate full IAM role arn string(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "isConfigCell": true,
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "bucket = 'summit-2020-db-ml'\n",
    "prefix = 'sagemaker/DEMO-gluon-recsys'\n",
    "\n",
    "import sagemaker\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the Python libraries we'll need for the remainder of this example notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, nd, ndarray\n",
    "from mxnet.metric import MSE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "from sagemaker.mxnet import MXNet\n",
    "import boto3\n",
    "import json\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data\n",
    "\n",
    "### Explore\n",
    "\n",
    "S3에서 실습 데이터를 로컬 스토리지로 가져옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir /tmp/recsys/\n",
    "# !aws s3 cp s3://summit-2020-db-ml/ecommerce-behavior-data/org/2019-Oct.csv /tmp/recsys/\n",
    "# !aws s3 cp s3://summit-2020-db-ml/ecommerce-behavior-data/org/2019-Nov.csv /tmp/recsys/\n",
    "    \n",
    "# !aws s3 cp s3://summit-2020-db-ml/ecommerce-behavior-data/temp/train/temp.csv /tmp/recsys/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the data into a [Pandas DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html) so that we can begin to understand it.\n",
    "\n",
    "*Note, we'll set `error_bad_lines=False` when reading the file in as there appear to be a very small number of records which would create a problem otherwise.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_time</th>\n",
       "      <th>event_type</th>\n",
       "      <th>product_id</th>\n",
       "      <th>category_id</th>\n",
       "      <th>category_code</th>\n",
       "      <th>brand</th>\n",
       "      <th>price</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_session</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-10-01 00:00:00 UTC</td>\n",
       "      <td>view</td>\n",
       "      <td>44600062</td>\n",
       "      <td>2103807459595387724</td>\n",
       "      <td>NaN</td>\n",
       "      <td>shiseido</td>\n",
       "      <td>35.79</td>\n",
       "      <td>541312140</td>\n",
       "      <td>72d76fde-8bb3-4e00-8c23-a032dfed738c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-10-01 00:00:00 UTC</td>\n",
       "      <td>view</td>\n",
       "      <td>3900821</td>\n",
       "      <td>2053013552326770905</td>\n",
       "      <td>appliances.environment.water_heater</td>\n",
       "      <td>aqua</td>\n",
       "      <td>33.20</td>\n",
       "      <td>554748717</td>\n",
       "      <td>9333dfbd-b87a-4708-9857-6336556b0fcc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-10-01 00:00:01 UTC</td>\n",
       "      <td>view</td>\n",
       "      <td>17200506</td>\n",
       "      <td>2053013559792632471</td>\n",
       "      <td>furniture.living_room.sofa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>543.10</td>\n",
       "      <td>519107250</td>\n",
       "      <td>566511c2-e2e3-422b-b695-cf8e6e792ca8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-10-01 00:00:01 UTC</td>\n",
       "      <td>view</td>\n",
       "      <td>1307067</td>\n",
       "      <td>2053013558920217191</td>\n",
       "      <td>computers.notebook</td>\n",
       "      <td>lenovo</td>\n",
       "      <td>251.74</td>\n",
       "      <td>550050854</td>\n",
       "      <td>7c90fc70-0e80-4590-96f3-13c02c18c713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-10-01 00:00:04 UTC</td>\n",
       "      <td>view</td>\n",
       "      <td>1004237</td>\n",
       "      <td>2053013555631882655</td>\n",
       "      <td>electronics.smartphone</td>\n",
       "      <td>apple</td>\n",
       "      <td>1081.98</td>\n",
       "      <td>535871217</td>\n",
       "      <td>c6bd7419-2748-4c56-95b4-8cec9ff8b80d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                event_time event_type  product_id          category_id  \\\n",
       "0  2019-10-01 00:00:00 UTC       view    44600062  2103807459595387724   \n",
       "1  2019-10-01 00:00:00 UTC       view     3900821  2053013552326770905   \n",
       "2  2019-10-01 00:00:01 UTC       view    17200506  2053013559792632471   \n",
       "3  2019-10-01 00:00:01 UTC       view     1307067  2053013558920217191   \n",
       "4  2019-10-01 00:00:04 UTC       view     1004237  2053013555631882655   \n",
       "\n",
       "                         category_code     brand    price    user_id  \\\n",
       "0                                  NaN  shiseido    35.79  541312140   \n",
       "1  appliances.environment.water_heater      aqua    33.20  554748717   \n",
       "2           furniture.living_room.sofa       NaN   543.10  519107250   \n",
       "3                   computers.notebook    lenovo   251.74  550050854   \n",
       "4               electronics.smartphone     apple  1081.98  535871217   \n",
       "\n",
       "                           user_session  \n",
       "0  72d76fde-8bb3-4e00-8c23-a032dfed738c  \n",
       "1  9333dfbd-b87a-4708-9857-6336556b0fcc  \n",
       "2  566511c2-e2e3-422b-b695-cf8e6e792ca8  \n",
       "3  7c90fc70-0e80-4590-96f3-13c02c18c713  \n",
       "4  c6bd7419-2748-4c56-95b4-8cec9ff8b80d  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.read_csv('/tmp/recsys/temp/temp.csv', delimiter=',',error_bad_lines=False)\n",
    "\n",
    "# df_Oct = pd.read_csv('/tmp/recsys/2019-Oct.csv', delimiter=',',error_bad_lines=False)\n",
    "# df_Nov = pd.read_csv('/tmp/recsys/2019-Nov.csv', delimiter=',',error_bad_lines=False)\n",
    "# df = pd.concat([df_Oct,df_Nov], ignore_index=True)\n",
    "\n",
    "df = pd.read_csv('/tmp/recsys/2019-Oct.csv', delimiter=',',error_bad_lines=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see this dataset includes information like:\n",
    "\n",
    "- `event_time`: When event is was happened (UTC)\n",
    "- `event_type`: Event type: one of [view, cart, remove_from_cart, purchase]\n",
    "- `product_id`: Product ID\n",
    "- `category_id`: Product category ID\n",
    "- `category_code`: Category meaningful name (if present)\n",
    "- `brand`: Brand name in lower case (if present)\n",
    "- `price`: Product price\n",
    "- `user_id`: Permanent user ID\n",
    "- `user_session`: User session ID\n",
    "\n",
    "이 예제에서는 `user_id`, `product_id`, `event_type`만 학습에 사용합니다. `category_id`, `category_code`, `brand`으로 `title` 컬럼을 만들어서 추후 결과 데이터를 확인하기 위해 사용합니다. `event_type`의 값 'view, cart, remove_from_cart, purchase'를 각각 0,1,2,3으로 수치화하여 `event_type_digit`를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['user_id', 'product_id', 'event_type', 'category_id', 'category_code', 'brand']]\n",
    "df['event_type_digit'] = df['event_type'].apply(lambda x: 4 if x=='purchase' else 3 if x=='cart' else 2 if x=='remove_from_cart' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['category_code'].isnull(), 'title'] = df['brand']\n",
    "df.loc[df['title'].isnull(), 'title'] = df['category_id']\n",
    "\n",
    "df = df[['user_id', 'product_id', 'event_type_digit', 'title']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because most people haven't viewed most products, and people purchase fewer products than we actually view, we'd expect our data to be sparse.  Our algorithm should work well with this sparse problem in general, but we may still want to clean out some of the long tail.  Let's look at some basic percentiles to confirm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users\n",
      " 0.00       1.0\n",
      "0.01       1.0\n",
      "0.02       1.0\n",
      "0.03       1.0\n",
      "0.04       1.0\n",
      "0.05       1.0\n",
      "0.10       1.0\n",
      "0.25       2.0\n",
      "0.50       4.0\n",
      "0.75      13.0\n",
      "0.90      34.0\n",
      "0.95      57.0\n",
      "0.96      65.0\n",
      "0.97      78.0\n",
      "0.98      98.0\n",
      "0.99     141.0\n",
      "1.00    7436.0\n",
      "Name: user_id, dtype: float64\n",
      "products\n",
      " 0.00         1.0\n",
      "0.01         1.0\n",
      "0.02         1.0\n",
      "0.03         1.0\n",
      "0.04         1.0\n",
      "0.05         1.0\n",
      "0.10         2.0\n",
      "0.25         7.0\n",
      "0.50        25.0\n",
      "0.75        92.0\n",
      "0.90       330.0\n",
      "0.95       740.0\n",
      "0.96       933.0\n",
      "0.97      1252.0\n",
      "0.98      1862.0\n",
      "0.99      3449.0\n",
      "1.00    500354.0\n",
      "Name: product_id, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "users = df['user_id'].value_counts()\n",
    "products = df['product_id'].value_counts()\n",
    "\n",
    "quantiles = [0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.96, 0.97, 0.98, 0.99, 1]\n",
    "print('users\\n', users.quantile(quantiles))\n",
    "print('products\\n', products.quantile(quantiles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean\n",
    "\n",
    "Let's filter out this long tail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = users[users >= 5]\n",
    "products = products[products >= 10]\n",
    "\n",
    "reduced_df = df.merge(pd.DataFrame({'user_id': users.index})).merge(pd.DataFrame({'product_id': products.index}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll recreate our customer and product lists since there are customers with more than 5 reviews, but all of their reviews are on products with less than 5 reviews (and vice versa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = reduced_df['user_id'].value_counts()\n",
    "products = reduced_df['product_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll number each user and item, giving them their own sequential index.  This will allow us to hold the information in a sparse format where the sequential indices indicate the row and column in our ratings matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>event_type_digit</th>\n",
       "      <th>title</th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>541312140</td>\n",
       "      <td>44600062</td>\n",
       "      <td>1</td>\n",
       "      <td>shiseido</td>\n",
       "      <td>48561</td>\n",
       "      <td>30881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>541312140</td>\n",
       "      <td>44600062</td>\n",
       "      <td>1</td>\n",
       "      <td>shiseido</td>\n",
       "      <td>48561</td>\n",
       "      <td>30881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>552720267</td>\n",
       "      <td>44600062</td>\n",
       "      <td>1</td>\n",
       "      <td>shiseido</td>\n",
       "      <td>2326</td>\n",
       "      <td>30881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>526625700</td>\n",
       "      <td>44600062</td>\n",
       "      <td>1</td>\n",
       "      <td>shiseido</td>\n",
       "      <td>1691</td>\n",
       "      <td>30881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>516251606</td>\n",
       "      <td>44600062</td>\n",
       "      <td>1</td>\n",
       "      <td>shiseido</td>\n",
       "      <td>377049</td>\n",
       "      <td>30881</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_id  product_id  event_type_digit     title    user   item\n",
       "0  541312140    44600062                 1  shiseido   48561  30881\n",
       "1  541312140    44600062                 1  shiseido   48561  30881\n",
       "2  552720267    44600062                 1  shiseido    2326  30881\n",
       "3  526625700    44600062                 1  shiseido    1691  30881\n",
       "4  516251606    44600062                 1  shiseido  377049  30881"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_index = pd.DataFrame({'user_id': users.index, 'user': np.arange(users.shape[0])})\n",
    "product_index = pd.DataFrame({'product_id': products.index, \n",
    "                              'item': np.arange(products.shape[0])})\n",
    "\n",
    "reduced_df = reduced_df.merge(user_index).merge(product_index)\n",
    "reduced_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare\n",
    "\n",
    "Let's start by splitting in training and test sets.  This will allow us to estimate the model's accuracy on videos our customers rated, but wasn't included in our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = reduced_df.groupby('user_id').last().reset_index()\n",
    "\n",
    "train_df = reduced_df.merge(test_df[['user_id', 'product_id']], \n",
    "                            on=['user_id', 'product_id'], \n",
    "                            how='outer', \n",
    "                            indicator=True)\n",
    "train_df = train_df[(train_df['_merge'] == 'left_only')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can convert our Pandas DataFrames into MXNet NDArrays, use those to create a member of the SparseMatrixDataset class, and add that to an MXNet Data Iterator.  This process is the same for both test and control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "\n",
    "train = gluon.data.ArrayDataset(nd.array(train_df['user'].values, dtype=np.float32),\n",
    "                                nd.array(train_df['item'].values, dtype=np.float32),\n",
    "                                nd.array(train_df['event_type_digit'].values, dtype=np.float32))\n",
    "test  = gluon.data.ArrayDataset(nd.array(test_df['user'].values, dtype=np.float32),\n",
    "                                nd.array(test_df['item'].values, dtype=np.float32),\n",
    "                                nd.array(test_df['event_type_digit'].values, dtype=np.float32))\n",
    "\n",
    "train_iter = gluon.data.DataLoader(train, shuffle=True, num_workers=4, batch_size=batch_size, last_batch='rollover')\n",
    "test_iter = gluon.data.DataLoader(train, shuffle=True, num_workers=4, batch_size=batch_size, last_batch='rollover')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Train Locally\n",
    "\n",
    "### Define Network\n",
    "\n",
    "Let's start by defining the neural network version of our matrix factorization task.  In this case, our network is quite simple.  The main components are:\n",
    "- [Embeddings](https://mxnet.incubator.apache.org/api/python/gluon/nn.html#mxnet.gluon.nn.Embedding) which turn our indexes into dense vectors of fixed size.  In this case, 64.\n",
    "- [Dense layers](https://mxnet.incubator.apache.org/api/python/gluon.html#mxnet.gluon.nn.Dense) with ReLU activation.  Each dense layer has the same number of units as our number of embeddings.  Our ReLU activation here also adds some non-linearity to our matrix factorization.\n",
    "- [Dropout layers](https://mxnet.incubator.apache.org/api/python/gluon.html#mxnet.gluon.nn.Dropout) which can be used to prevent over-fitting.\n",
    "- Matrix multiplication of our user matrix and our item matrix to create an estimate of our rating matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFBlock(gluon.HybridBlock):\n",
    "    def __init__(self, max_users, max_items, num_emb, dropout_p=0.5):\n",
    "        super(MFBlock, self).__init__()\n",
    "        \n",
    "        self.max_users = max_users\n",
    "        self.max_items = max_items\n",
    "        self.dropout_p = dropout_p\n",
    "        self.num_emb = num_emb\n",
    "        \n",
    "        with self.name_scope():\n",
    "            self.user_embeddings = gluon.nn.Embedding(max_users, num_emb)\n",
    "            self.item_embeddings = gluon.nn.Embedding(max_items, num_emb)\n",
    "            \n",
    "            self.dropout_user = gluon.nn.Dropout(dropout_p)\n",
    "            self.dropout_item = gluon.nn.Dropout(dropout_p)\n",
    "\n",
    "            self.dense_user   = gluon.nn.Dense(num_emb, activation='relu')\n",
    "            self.dense_item = gluon.nn.Dense(num_emb, activation='relu')\n",
    "            \n",
    "    def hybrid_forward(self, F, users, items):\n",
    "        a = self.user_embeddings(users)\n",
    "        a = self.dense_user(a)\n",
    "        \n",
    "        b = self.item_embeddings(items)\n",
    "        b = self.dense_item(b)\n",
    "\n",
    "        predictions = self.dropout_user(a) * self.dropout_item(b)     \n",
    "        predictions = F.sum(predictions, axis=1)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_embeddings = 64\n",
    "\n",
    "net = MFBlock(max_users=user_index.shape[0], \n",
    "              max_items=product_index.shape[0],\n",
    "              num_emb=num_embeddings,\n",
    "              dropout_p=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Parameters\n",
    "\n",
    "Let's initialize network weights and set our optimization parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize network parameters\n",
    "ctx = mx.gpu()\n",
    "net.collect_params().initialize(mx.init.Xavier(magnitude=60),\n",
    "                                ctx=ctx,\n",
    "                                force_reinit=True)\n",
    "net.hybridize()\n",
    "\n",
    "# Set optimization parameters\n",
    "opt = 'sgd'\n",
    "lr = 0.02\n",
    "momentum = 0.9\n",
    "wd = 0.\n",
    "\n",
    "trainer = gluon.Trainer(net.collect_params(),\n",
    "                        opt,\n",
    "                        {'learning_rate': lr,\n",
    "                         'wd': wd,\n",
    "                         'momentum': momentum})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute\n",
    "\n",
    "Let's define a function to carry out the training of our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute(train_iter, test_iter, net, epochs, ctx):\n",
    "    \n",
    "    loss_function = gluon.loss.L2Loss()\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        print(\"epoch: {}\".format(e))\n",
    "        \n",
    "        for i, (user, item, label) in enumerate(train_iter):\n",
    "                user = user.as_in_context(ctx)\n",
    "                item = item.as_in_context(ctx)\n",
    "                label = label.as_in_context(ctx)\n",
    "                \n",
    "                with mx.autograd.record():\n",
    "                    output = net(user, item)               \n",
    "                    loss = loss_function(output, label)\n",
    "                    \n",
    "                loss.backward()\n",
    "                trainer.step(batch_size)\n",
    "\n",
    "        print(\"EPOCH {}: MSE ON TRAINING and TEST: {}. {}\".format(e,\n",
    "                                                                   eval_net(train_iter, net, ctx, loss_function),\n",
    "                                                                   eval_net(test_iter, net, ctx, loss_function)))\n",
    "    print(\"end of training\")\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also define a function which evaluates our network on a given dataset.  This is called by our `execute` function above to provide mean squared error values on our training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_net(data, net, ctx, loss_function):\n",
    "    acc = MSE()\n",
    "    for i, (user, item, label) in enumerate(data):\n",
    "        \n",
    "            user = user.as_in_context(ctx)\n",
    "            item = item.as_in_context(ctx)\n",
    "            label = label.as_in_context(ctx)\n",
    "            predictions = net(user, item).reshape((batch_size, 1))\n",
    "            acc.update(preds=[predictions], labels=[label])\n",
    "   \n",
    "    return acc.get()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train for a few epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "EPOCH 0: MSE ON TRAINING and TEST: 0.20644457884360345. 0.2064442551470329\n",
      "epoch: 1\n",
      "EPOCH 1: MSE ON TRAINING and TEST: 0.19942977052587943. 0.19943039569846346\n",
      "epoch: 2\n",
      "EPOCH 2: MSE ON TRAINING and TEST: 0.19601205106186695. 0.19600894609423372\n",
      "end of training\n",
      "CPU times: user 40min 54s, sys: 6min 15s, total: 47min 10s\n",
      "Wall time: 54min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "trained_net = execute(train_iter, test_iter, net, epochs, ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Early Validation\n",
    "\n",
    "We can see our training error going down, but our validation accuracy bounces around a bit.  Let's check how our model is predicting for an individual user.  We could pick randomly, but for this case, let's try user #6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>item</th>\n",
       "      <th>u6_predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1004856</td>\n",
       "      <td>0</td>\n",
       "      <td>1.201158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>10701101</td>\n",
       "      <td>269</td>\n",
       "      <td>1.176389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1004858</td>\n",
       "      <td>31</td>\n",
       "      <td>1.167507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4804056</td>\n",
       "      <td>6</td>\n",
       "      <td>1.153852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5100816</td>\n",
       "      <td>9</td>\n",
       "      <td>1.148042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1531</th>\n",
       "      <td>12700692</td>\n",
       "      <td>1531</td>\n",
       "      <td>1.143239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1004870</td>\n",
       "      <td>5</td>\n",
       "      <td>1.142744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1004833</td>\n",
       "      <td>3</td>\n",
       "      <td>1.138810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1412</th>\n",
       "      <td>1005207</td>\n",
       "      <td>1412</td>\n",
       "      <td>1.130470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6284</th>\n",
       "      <td>12710856</td>\n",
       "      <td>6284</td>\n",
       "      <td>1.124848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3761</th>\n",
       "      <td>10700955</td>\n",
       "      <td>3761</td>\n",
       "      <td>1.124306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>1004863</td>\n",
       "      <td>93</td>\n",
       "      <td>1.123279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1004767</td>\n",
       "      <td>1</td>\n",
       "      <td>1.123161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>4802159</td>\n",
       "      <td>480</td>\n",
       "      <td>1.122590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>1004839</td>\n",
       "      <td>75</td>\n",
       "      <td>1.121628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1004838</td>\n",
       "      <td>22</td>\n",
       "      <td>1.120352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>12701602</td>\n",
       "      <td>682</td>\n",
       "      <td>1.120086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1002547</td>\n",
       "      <td>86</td>\n",
       "      <td>1.118991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1002544</td>\n",
       "      <td>8</td>\n",
       "      <td>1.118979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1004873</td>\n",
       "      <td>12</td>\n",
       "      <td>1.117452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>1004871</td>\n",
       "      <td>124</td>\n",
       "      <td>1.117451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1004209</td>\n",
       "      <td>39</td>\n",
       "      <td>1.117167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1005100</td>\n",
       "      <td>17</td>\n",
       "      <td>1.115004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1004750</td>\n",
       "      <td>19</td>\n",
       "      <td>1.113782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>1004834</td>\n",
       "      <td>72</td>\n",
       "      <td>1.113584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1556</th>\n",
       "      <td>3701277</td>\n",
       "      <td>1556</td>\n",
       "      <td>1.112914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1002633</td>\n",
       "      <td>20</td>\n",
       "      <td>1.111118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1004768</td>\n",
       "      <td>36</td>\n",
       "      <td>1.107765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>1005209</td>\n",
       "      <td>519</td>\n",
       "      <td>1.107051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>1005099</td>\n",
       "      <td>116</td>\n",
       "      <td>1.105484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70747</th>\n",
       "      <td>7900603</td>\n",
       "      <td>70747</td>\n",
       "      <td>0.823055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94496</th>\n",
       "      <td>6801415</td>\n",
       "      <td>94496</td>\n",
       "      <td>0.823044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99241</th>\n",
       "      <td>25100292</td>\n",
       "      <td>99241</td>\n",
       "      <td>0.822778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60928</th>\n",
       "      <td>12701295</td>\n",
       "      <td>60928</td>\n",
       "      <td>0.822669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43919</th>\n",
       "      <td>6800214</td>\n",
       "      <td>43919</td>\n",
       "      <td>0.822450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94522</th>\n",
       "      <td>26004000</td>\n",
       "      <td>94522</td>\n",
       "      <td>0.822388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66531</th>\n",
       "      <td>20100240</td>\n",
       "      <td>66531</td>\n",
       "      <td>0.822295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96804</th>\n",
       "      <td>26005952</td>\n",
       "      <td>96804</td>\n",
       "      <td>0.821140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88566</th>\n",
       "      <td>12706834</td>\n",
       "      <td>88566</td>\n",
       "      <td>0.820885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115902</th>\n",
       "      <td>25508333</td>\n",
       "      <td>115902</td>\n",
       "      <td>0.820871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92116</th>\n",
       "      <td>2500998</td>\n",
       "      <td>92116</td>\n",
       "      <td>0.819969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96519</th>\n",
       "      <td>6801181</td>\n",
       "      <td>96519</td>\n",
       "      <td>0.819568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104914</th>\n",
       "      <td>3100949</td>\n",
       "      <td>104914</td>\n",
       "      <td>0.819313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97354</th>\n",
       "      <td>12400214</td>\n",
       "      <td>97354</td>\n",
       "      <td>0.819093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92323</th>\n",
       "      <td>1701450</td>\n",
       "      <td>92323</td>\n",
       "      <td>0.818200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108727</th>\n",
       "      <td>21401641</td>\n",
       "      <td>108727</td>\n",
       "      <td>0.817998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50728</th>\n",
       "      <td>21410214</td>\n",
       "      <td>50728</td>\n",
       "      <td>0.817864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98376</th>\n",
       "      <td>6902857</td>\n",
       "      <td>98376</td>\n",
       "      <td>0.816011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67176</th>\n",
       "      <td>6902478</td>\n",
       "      <td>67176</td>\n",
       "      <td>0.815417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98782</th>\n",
       "      <td>17700608</td>\n",
       "      <td>98782</td>\n",
       "      <td>0.815149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68451</th>\n",
       "      <td>27201176</td>\n",
       "      <td>68451</td>\n",
       "      <td>0.815047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109881</th>\n",
       "      <td>16700437</td>\n",
       "      <td>109881</td>\n",
       "      <td>0.813889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91376</th>\n",
       "      <td>9002179</td>\n",
       "      <td>91376</td>\n",
       "      <td>0.813560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102487</th>\n",
       "      <td>26021860</td>\n",
       "      <td>102487</td>\n",
       "      <td>0.812834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109961</th>\n",
       "      <td>26018708</td>\n",
       "      <td>109961</td>\n",
       "      <td>0.812740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70110</th>\n",
       "      <td>4501654</td>\n",
       "      <td>70110</td>\n",
       "      <td>0.811847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97223</th>\n",
       "      <td>26404249</td>\n",
       "      <td>97223</td>\n",
       "      <td>0.811316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78943</th>\n",
       "      <td>25300137</td>\n",
       "      <td>78943</td>\n",
       "      <td>0.809192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54962</th>\n",
       "      <td>12716072</td>\n",
       "      <td>54962</td>\n",
       "      <td>0.805522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104112</th>\n",
       "      <td>12300130</td>\n",
       "      <td>104112</td>\n",
       "      <td>0.777543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116081 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        product_id    item  u6_predictions\n",
       "0          1004856       0        1.201158\n",
       "269       10701101     269        1.176389\n",
       "31         1004858      31        1.167507\n",
       "6          4804056       6        1.153852\n",
       "9          5100816       9        1.148042\n",
       "1531      12700692    1531        1.143239\n",
       "5          1004870       5        1.142744\n",
       "3          1004833       3        1.138810\n",
       "1412       1005207    1412        1.130470\n",
       "6284      12710856    6284        1.124848\n",
       "3761      10700955    3761        1.124306\n",
       "93         1004863      93        1.123279\n",
       "1          1004767       1        1.123161\n",
       "480        4802159     480        1.122590\n",
       "75         1004839      75        1.121628\n",
       "22         1004838      22        1.120352\n",
       "682       12701602     682        1.120086\n",
       "86         1002547      86        1.118991\n",
       "8          1002544       8        1.118979\n",
       "12         1004873      12        1.117452\n",
       "124        1004871     124        1.117451\n",
       "39         1004209      39        1.117167\n",
       "17         1005100      17        1.115004\n",
       "19         1004750      19        1.113782\n",
       "72         1004834      72        1.113584\n",
       "1556       3701277    1556        1.112914\n",
       "20         1002633      20        1.111118\n",
       "36         1004768      36        1.107765\n",
       "519        1005209     519        1.107051\n",
       "116        1005099     116        1.105484\n",
       "...            ...     ...             ...\n",
       "70747      7900603   70747        0.823055\n",
       "94496      6801415   94496        0.823044\n",
       "99241     25100292   99241        0.822778\n",
       "60928     12701295   60928        0.822669\n",
       "43919      6800214   43919        0.822450\n",
       "94522     26004000   94522        0.822388\n",
       "66531     20100240   66531        0.822295\n",
       "96804     26005952   96804        0.821140\n",
       "88566     12706834   88566        0.820885\n",
       "115902    25508333  115902        0.820871\n",
       "92116      2500998   92116        0.819969\n",
       "96519      6801181   96519        0.819568\n",
       "104914     3100949  104914        0.819313\n",
       "97354     12400214   97354        0.819093\n",
       "92323      1701450   92323        0.818200\n",
       "108727    21401641  108727        0.817998\n",
       "50728     21410214   50728        0.817864\n",
       "98376      6902857   98376        0.816011\n",
       "67176      6902478   67176        0.815417\n",
       "98782     17700608   98782        0.815149\n",
       "68451     27201176   68451        0.815047\n",
       "109881    16700437  109881        0.813889\n",
       "91376      9002179   91376        0.813560\n",
       "102487    26021860  102487        0.812834\n",
       "109961    26018708  109961        0.812740\n",
       "70110      4501654   70110        0.811847\n",
       "97223     26404249   97223        0.811316\n",
       "78943     25300137   78943        0.809192\n",
       "54962     12716072   54962        0.805522\n",
       "104112    12300130  104112        0.777543\n",
       "\n",
       "[116081 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_index['u6_predictions'] = trained_net(nd.array([6] * product_index.shape[0]).as_in_context(ctx), \n",
    "                                              nd.array(product_index['item'].values).as_in_context(ctx)).asnumpy()\n",
    "product_index.sort_values('u6_predictions', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare this to the predictions for another user (we'll try user #7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>item</th>\n",
       "      <th>u6_predictions</th>\n",
       "      <th>u7_predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1004856</td>\n",
       "      <td>0</td>\n",
       "      <td>1.201158</td>\n",
       "      <td>1.241085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>10701101</td>\n",
       "      <td>269</td>\n",
       "      <td>1.176389</td>\n",
       "      <td>1.222679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1004858</td>\n",
       "      <td>31</td>\n",
       "      <td>1.167507</td>\n",
       "      <td>1.205321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4804056</td>\n",
       "      <td>6</td>\n",
       "      <td>1.153852</td>\n",
       "      <td>1.199182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5100816</td>\n",
       "      <td>9</td>\n",
       "      <td>1.148042</td>\n",
       "      <td>1.188282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1004870</td>\n",
       "      <td>5</td>\n",
       "      <td>1.142744</td>\n",
       "      <td>1.187881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1531</th>\n",
       "      <td>12700692</td>\n",
       "      <td>1531</td>\n",
       "      <td>1.143239</td>\n",
       "      <td>1.183559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6284</th>\n",
       "      <td>12710856</td>\n",
       "      <td>6284</td>\n",
       "      <td>1.124848</td>\n",
       "      <td>1.175246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1412</th>\n",
       "      <td>1005207</td>\n",
       "      <td>1412</td>\n",
       "      <td>1.130470</td>\n",
       "      <td>1.173594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1004833</td>\n",
       "      <td>3</td>\n",
       "      <td>1.138810</td>\n",
       "      <td>1.165914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>1004863</td>\n",
       "      <td>93</td>\n",
       "      <td>1.123279</td>\n",
       "      <td>1.165576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3761</th>\n",
       "      <td>10700955</td>\n",
       "      <td>3761</td>\n",
       "      <td>1.124306</td>\n",
       "      <td>1.161602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1004873</td>\n",
       "      <td>12</td>\n",
       "      <td>1.117452</td>\n",
       "      <td>1.159535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1002544</td>\n",
       "      <td>8</td>\n",
       "      <td>1.118979</td>\n",
       "      <td>1.159064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>12701602</td>\n",
       "      <td>682</td>\n",
       "      <td>1.120086</td>\n",
       "      <td>1.159049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1004838</td>\n",
       "      <td>22</td>\n",
       "      <td>1.120352</td>\n",
       "      <td>1.157855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1002547</td>\n",
       "      <td>86</td>\n",
       "      <td>1.118991</td>\n",
       "      <td>1.157734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1004767</td>\n",
       "      <td>1</td>\n",
       "      <td>1.123161</td>\n",
       "      <td>1.155863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>1004839</td>\n",
       "      <td>75</td>\n",
       "      <td>1.121628</td>\n",
       "      <td>1.154131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>4802159</td>\n",
       "      <td>480</td>\n",
       "      <td>1.122590</td>\n",
       "      <td>1.153096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1002633</td>\n",
       "      <td>20</td>\n",
       "      <td>1.111118</td>\n",
       "      <td>1.152027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>1004871</td>\n",
       "      <td>124</td>\n",
       "      <td>1.117451</td>\n",
       "      <td>1.151653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1004209</td>\n",
       "      <td>39</td>\n",
       "      <td>1.117167</td>\n",
       "      <td>1.150782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1004750</td>\n",
       "      <td>19</td>\n",
       "      <td>1.113782</td>\n",
       "      <td>1.149267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>1004834</td>\n",
       "      <td>72</td>\n",
       "      <td>1.113584</td>\n",
       "      <td>1.147566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>1005209</td>\n",
       "      <td>519</td>\n",
       "      <td>1.107051</td>\n",
       "      <td>1.146019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>6300465</td>\n",
       "      <td>991</td>\n",
       "      <td>1.104115</td>\n",
       "      <td>1.145823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1005100</td>\n",
       "      <td>17</td>\n",
       "      <td>1.115004</td>\n",
       "      <td>1.143842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1004768</td>\n",
       "      <td>36</td>\n",
       "      <td>1.107765</td>\n",
       "      <td>1.142045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1556</th>\n",
       "      <td>3701277</td>\n",
       "      <td>1556</td>\n",
       "      <td>1.112914</td>\n",
       "      <td>1.141973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108727</th>\n",
       "      <td>21401641</td>\n",
       "      <td>108727</td>\n",
       "      <td>0.817998</td>\n",
       "      <td>0.848289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111391</th>\n",
       "      <td>16100521</td>\n",
       "      <td>111391</td>\n",
       "      <td>0.825286</td>\n",
       "      <td>0.848154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78943</th>\n",
       "      <td>25300137</td>\n",
       "      <td>78943</td>\n",
       "      <td>0.809192</td>\n",
       "      <td>0.848071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114563</th>\n",
       "      <td>26027113</td>\n",
       "      <td>114563</td>\n",
       "      <td>0.823582</td>\n",
       "      <td>0.848031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105246</th>\n",
       "      <td>9100913</td>\n",
       "      <td>105246</td>\n",
       "      <td>0.831865</td>\n",
       "      <td>0.847749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100371</th>\n",
       "      <td>45600854</td>\n",
       "      <td>100371</td>\n",
       "      <td>0.826162</td>\n",
       "      <td>0.847437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94496</th>\n",
       "      <td>6801415</td>\n",
       "      <td>94496</td>\n",
       "      <td>0.823044</td>\n",
       "      <td>0.847422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73335</th>\n",
       "      <td>44600169</td>\n",
       "      <td>73335</td>\n",
       "      <td>0.833982</td>\n",
       "      <td>0.846551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112426</th>\n",
       "      <td>5301655</td>\n",
       "      <td>112426</td>\n",
       "      <td>0.823434</td>\n",
       "      <td>0.846131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101071</th>\n",
       "      <td>26404295</td>\n",
       "      <td>101071</td>\n",
       "      <td>0.838107</td>\n",
       "      <td>0.846128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92116</th>\n",
       "      <td>2500998</td>\n",
       "      <td>92116</td>\n",
       "      <td>0.819969</td>\n",
       "      <td>0.845169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94522</th>\n",
       "      <td>26004000</td>\n",
       "      <td>94522</td>\n",
       "      <td>0.822388</td>\n",
       "      <td>0.844776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98782</th>\n",
       "      <td>17700608</td>\n",
       "      <td>98782</td>\n",
       "      <td>0.815149</td>\n",
       "      <td>0.843727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91376</th>\n",
       "      <td>9002179</td>\n",
       "      <td>91376</td>\n",
       "      <td>0.813560</td>\n",
       "      <td>0.843638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95840</th>\n",
       "      <td>18900078</td>\n",
       "      <td>95840</td>\n",
       "      <td>0.830153</td>\n",
       "      <td>0.843635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77179</th>\n",
       "      <td>31501035</td>\n",
       "      <td>77179</td>\n",
       "      <td>0.825409</td>\n",
       "      <td>0.843000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70110</th>\n",
       "      <td>4501654</td>\n",
       "      <td>70110</td>\n",
       "      <td>0.811847</td>\n",
       "      <td>0.842145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102487</th>\n",
       "      <td>26021860</td>\n",
       "      <td>102487</td>\n",
       "      <td>0.812834</td>\n",
       "      <td>0.840434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99241</th>\n",
       "      <td>25100292</td>\n",
       "      <td>99241</td>\n",
       "      <td>0.822778</td>\n",
       "      <td>0.840374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97223</th>\n",
       "      <td>26404249</td>\n",
       "      <td>97223</td>\n",
       "      <td>0.811316</td>\n",
       "      <td>0.839678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96804</th>\n",
       "      <td>26005952</td>\n",
       "      <td>96804</td>\n",
       "      <td>0.821140</td>\n",
       "      <td>0.839578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109961</th>\n",
       "      <td>26018708</td>\n",
       "      <td>109961</td>\n",
       "      <td>0.812740</td>\n",
       "      <td>0.838325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98376</th>\n",
       "      <td>6902857</td>\n",
       "      <td>98376</td>\n",
       "      <td>0.816011</td>\n",
       "      <td>0.837964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68760</th>\n",
       "      <td>17500627</td>\n",
       "      <td>68760</td>\n",
       "      <td>0.823392</td>\n",
       "      <td>0.837905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54962</th>\n",
       "      <td>12716072</td>\n",
       "      <td>54962</td>\n",
       "      <td>0.805522</td>\n",
       "      <td>0.836724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114318</th>\n",
       "      <td>28721057</td>\n",
       "      <td>114318</td>\n",
       "      <td>0.830773</td>\n",
       "      <td>0.835517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98574</th>\n",
       "      <td>8500463</td>\n",
       "      <td>98574</td>\n",
       "      <td>0.824521</td>\n",
       "      <td>0.833046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97354</th>\n",
       "      <td>12400214</td>\n",
       "      <td>97354</td>\n",
       "      <td>0.819093</td>\n",
       "      <td>0.829773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96519</th>\n",
       "      <td>6801181</td>\n",
       "      <td>96519</td>\n",
       "      <td>0.819568</td>\n",
       "      <td>0.823326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104112</th>\n",
       "      <td>12300130</td>\n",
       "      <td>104112</td>\n",
       "      <td>0.777543</td>\n",
       "      <td>0.791545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116081 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        product_id    item  u6_predictions  u7_predictions\n",
       "0          1004856       0        1.201158        1.241085\n",
       "269       10701101     269        1.176389        1.222679\n",
       "31         1004858      31        1.167507        1.205321\n",
       "6          4804056       6        1.153852        1.199182\n",
       "9          5100816       9        1.148042        1.188282\n",
       "5          1004870       5        1.142744        1.187881\n",
       "1531      12700692    1531        1.143239        1.183559\n",
       "6284      12710856    6284        1.124848        1.175246\n",
       "1412       1005207    1412        1.130470        1.173594\n",
       "3          1004833       3        1.138810        1.165914\n",
       "93         1004863      93        1.123279        1.165576\n",
       "3761      10700955    3761        1.124306        1.161602\n",
       "12         1004873      12        1.117452        1.159535\n",
       "8          1002544       8        1.118979        1.159064\n",
       "682       12701602     682        1.120086        1.159049\n",
       "22         1004838      22        1.120352        1.157855\n",
       "86         1002547      86        1.118991        1.157734\n",
       "1          1004767       1        1.123161        1.155863\n",
       "75         1004839      75        1.121628        1.154131\n",
       "480        4802159     480        1.122590        1.153096\n",
       "20         1002633      20        1.111118        1.152027\n",
       "124        1004871     124        1.117451        1.151653\n",
       "39         1004209      39        1.117167        1.150782\n",
       "19         1004750      19        1.113782        1.149267\n",
       "72         1004834      72        1.113584        1.147566\n",
       "519        1005209     519        1.107051        1.146019\n",
       "991        6300465     991        1.104115        1.145823\n",
       "17         1005100      17        1.115004        1.143842\n",
       "36         1004768      36        1.107765        1.142045\n",
       "1556       3701277    1556        1.112914        1.141973\n",
       "...            ...     ...             ...             ...\n",
       "108727    21401641  108727        0.817998        0.848289\n",
       "111391    16100521  111391        0.825286        0.848154\n",
       "78943     25300137   78943        0.809192        0.848071\n",
       "114563    26027113  114563        0.823582        0.848031\n",
       "105246     9100913  105246        0.831865        0.847749\n",
       "100371    45600854  100371        0.826162        0.847437\n",
       "94496      6801415   94496        0.823044        0.847422\n",
       "73335     44600169   73335        0.833982        0.846551\n",
       "112426     5301655  112426        0.823434        0.846131\n",
       "101071    26404295  101071        0.838107        0.846128\n",
       "92116      2500998   92116        0.819969        0.845169\n",
       "94522     26004000   94522        0.822388        0.844776\n",
       "98782     17700608   98782        0.815149        0.843727\n",
       "91376      9002179   91376        0.813560        0.843638\n",
       "95840     18900078   95840        0.830153        0.843635\n",
       "77179     31501035   77179        0.825409        0.843000\n",
       "70110      4501654   70110        0.811847        0.842145\n",
       "102487    26021860  102487        0.812834        0.840434\n",
       "99241     25100292   99241        0.822778        0.840374\n",
       "97223     26404249   97223        0.811316        0.839678\n",
       "96804     26005952   96804        0.821140        0.839578\n",
       "109961    26018708  109961        0.812740        0.838325\n",
       "98376      6902857   98376        0.816011        0.837964\n",
       "68760     17500627   68760        0.823392        0.837905\n",
       "54962     12716072   54962        0.805522        0.836724\n",
       "114318    28721057  114318        0.830773        0.835517\n",
       "98574      8500463   98574        0.824521        0.833046\n",
       "97354     12400214   97354        0.819093        0.829773\n",
       "96519      6801181   96519        0.819568        0.823326\n",
       "104112    12300130  104112        0.777543        0.791545\n",
       "\n",
       "[116081 rows x 4 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_index['u7_predictions'] = trained_net(nd.array([7] * product_index.shape[0]).as_in_context(ctx), \n",
    "                                              nd.array(product_index['item'].values).as_in_context(ctx)).asnumpy()\n",
    "product_index.sort_values('u7_predictions', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted ratings are different between the two users, but the same top (and bottom) items for user #6 appear for #7 as well.  Let's look at the correlation across the full set of 38K items to see if this relationship holds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "product_index[['u6_predictions', 'u7_predictions']].plot.scatter('u6_predictions', 'u7_predictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this correlation is nearly perfect.  Essentially the average rating of items dominates across users and we'll recommend the same well-reviewed items to everyone.  As it turns out, we can add more embeddings and this relationship will go away since we're better able to capture differential preferences across users.\n",
    "\n",
    "If we ran this outside of our Notebook Instance we could run larger jobs and move on to other work would improve productivity.\n",
    "\n",
    "---\n",
    "\n",
    "## Train with SageMaker\n",
    "\n",
    "Now that we've trained on this smaller dataset, we can expand training in SageMaker's distributed, managed training environment.\n",
    "\n",
    "### Wrap Code\n",
    "\n",
    "To use SageMaker's pre-built MXNet container, we'll need to wrap our code from above into a Python script.  There's a great deal of flexibility in using SageMaker's pre-built containers, and detailed documentation can be found [here](https://github.com/aws/sagemaker-python-sdk#mxnet-sagemaker-estimators), but for our example, it consisted of:\n",
    "1. Wrapping all data preparation into a `prepare_train_data` function (we could name this whatever we like)\n",
    "1. Copying and pasting classes and functions from above word-for-word\n",
    "1. Defining a `train` function that:\n",
    "  1. Adds a bit of new code to pick up the input TSV dataset on the SageMaker Training cluster\n",
    "  1. Takes in a dict of hyperparameters (which we specified as globals above)\n",
    "  1. Creates the net and executes training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import logging\r\n",
      "import json\r\n",
      "import time\r\n",
      "import os\r\n",
      "import mxnet as mx\r\n",
      "from mxnet import gluon, nd, ndarray\r\n",
      "from mxnet.metric import MSE\r\n",
      "import numpy as np\r\n",
      "\r\n",
      "os.system('pip install pandas')\r\n",
      "import pandas as pd\r\n",
      "\r\n",
      "logging.basicConfig(level=logging.DEBUG)\r\n",
      "\r\n",
      "#########\r\n",
      "# Globals\r\n",
      "#########\r\n",
      "\r\n",
      "batch_size = 1024\r\n",
      "\r\n",
      "\r\n",
      "##########\r\n",
      "# Training\r\n",
      "##########\r\n",
      "\r\n",
      "def train(channel_input_dirs, hyperparameters, hosts, num_gpus, **kwargs):\r\n",
      "    \r\n",
      "    # get data\r\n",
      "    training_dir = channel_input_dirs['train']\r\n",
      "    train_iter, test_iter, user_index, product_index = prepare_train_data(training_dir)\r\n",
      "    \r\n",
      "    # get hyperparameters\r\n",
      "    num_embeddings = hyperparameters.get('num_embeddings', 64)\r\n",
      "    opt = hyperparameters.get('opt', 'sgd')\r\n",
      "    lr = hyperparameters.get('lr', 0.02)\r\n",
      "    momentum = hyperparameters.get('momentum', 0.9)\r\n",
      "    wd = hyperparameters.get('wd', 0.)\r\n",
      "    epochs = hyperparameters.get('epochs', 5)\r\n",
      "\r\n",
      "    # define net\r\n",
      "    ctx = mx.gpu()\r\n",
      "\r\n",
      "    net = MFBlock(max_users=user_index.shape[0], \r\n",
      "                  max_items=product_index.shape[0],\r\n",
      "                  num_emb=num_embeddings,\r\n",
      "                  dropout_p=0.5)\r\n",
      "    \r\n",
      "    net.collect_params().initialize(mx.init.Xavier(magnitude=60),\r\n",
      "                                    ctx=ctx,\r\n",
      "                                    force_reinit=True)\r\n",
      "    net.hybridize()\r\n",
      "\r\n",
      "    trainer = gluon.Trainer(net.collect_params(),\r\n",
      "                            opt,\r\n",
      "                            {'learning_rate': lr,\r\n",
      "                             'wd': wd,\r\n",
      "                             'momentum': momentum})\r\n",
      "    \r\n",
      "    # execute\r\n",
      "    trained_net = execute(train_iter, test_iter, net, trainer, epochs, ctx)\r\n",
      "    \r\n",
      "    return trained_net, user_index, product_index\r\n",
      "\r\n",
      "\r\n",
      "class MFBlock(gluon.HybridBlock):\r\n",
      "    def __init__(self, max_users, max_items, num_emb, dropout_p=0.5):\r\n",
      "        super(MFBlock, self).__init__()\r\n",
      "        \r\n",
      "        self.max_users = max_users\r\n",
      "        self.max_items = max_items\r\n",
      "        self.dropout_p = dropout_p\r\n",
      "        self.num_emb = num_emb\r\n",
      "        \r\n",
      "        with self.name_scope():\r\n",
      "            self.user_embeddings = gluon.nn.Embedding(max_users, num_emb)\r\n",
      "            self.item_embeddings = gluon.nn.Embedding(max_items, num_emb)\r\n",
      "\r\n",
      "            self.dropout_user = gluon.nn.Dropout(dropout_p)\r\n",
      "            self.dropout_item = gluon.nn.Dropout(dropout_p)\r\n",
      "\r\n",
      "            self.dense_user   = gluon.nn.Dense(num_emb, activation='relu')\r\n",
      "            self.dense_item = gluon.nn.Dense(num_emb, activation='relu')\r\n",
      "            \r\n",
      "    def hybrid_forward(self, F, users, items):\r\n",
      "        a = self.user_embeddings(users)\r\n",
      "        a = self.dense_user(a)\r\n",
      "        \r\n",
      "        b = self.item_embeddings(items)\r\n",
      "        b = self.dense_item(b)\r\n",
      "\r\n",
      "        predictions = self.dropout_user(a) * self.dropout_item(b)      \r\n",
      "        predictions = F.sum(predictions, axis=1)\r\n",
      "\r\n",
      "        return predictions\r\n",
      "\r\n",
      "    \r\n",
      "def execute(train_iter, test_iter, net, trainer, epochs, ctx):\r\n",
      "    loss_function = gluon.loss.L2Loss()\r\n",
      "    for e in range(epochs):\r\n",
      "        print(\"epoch: {}\".format(e))\r\n",
      "        for i, (user, item, label) in enumerate(train_iter):\r\n",
      "\r\n",
      "                user = user.as_in_context(ctx)\r\n",
      "                item = item.as_in_context(ctx)\r\n",
      "                label = label.as_in_context(ctx)\r\n",
      "\r\n",
      "                with mx.autograd.record():\r\n",
      "                    output = net(user, item)               \r\n",
      "                    loss = loss_function(output, label)\r\n",
      "                loss.backward()\r\n",
      "                trainer.step(batch_size)\r\n",
      "\r\n",
      "        print(\"EPOCH {}: MSE ON TRAINING and TEST: {}. {}\".format(e,\r\n",
      "                                                                   eval_net(train_iter, net, ctx, loss_function),\r\n",
      "                                                                   eval_net(test_iter, net, ctx, loss_function)))\r\n",
      "    print(\"end of training\")\r\n",
      "    return net\r\n",
      "\r\n",
      "\r\n",
      "def eval_net(data, net, ctx, loss_function):\r\n",
      "    acc = MSE()\r\n",
      "    for i, (user, item, label) in enumerate(data):\r\n",
      "\r\n",
      "            user = user.as_in_context(ctx)\r\n",
      "            item = item.as_in_context(ctx)\r\n",
      "            label = label.as_in_context(ctx)\r\n",
      "\r\n",
      "            predictions = net(user, item).reshape((batch_size, 1))\r\n",
      "            acc.update(preds=[predictions], labels=[label])\r\n",
      "\r\n",
      "    return acc.get()[1]\r\n",
      "\r\n",
      "\r\n",
      "def save(model, model_dir):\r\n",
      "    net, user_index, product_index = model\r\n",
      "    net.save_params('{}/model.params'.format(model_dir))\r\n",
      "    f = open('{}/MFBlock.params'.format(model_dir), 'w')\r\n",
      "    json.dump({'max_users': net.max_users,\r\n",
      "               'max_items': net.max_items,\r\n",
      "               'num_emb': net.num_emb,\r\n",
      "               'dropout_p': net.dropout_p},\r\n",
      "              f)\r\n",
      "    f.close()\r\n",
      "    user_index.to_csv('{}/user_index.csv'.format(model_dir), index=False)\r\n",
      "    product_index.to_csv('{}/product_index.csv'.format(model_dir), index=False)\r\n",
      "\r\n",
      "    \r\n",
      "######\r\n",
      "# Data\r\n",
      "######\r\n",
      "\r\n",
      "def prepare_train_data(training_dir):\r\n",
      "    f = os.listdir(training_dir)\r\n",
      "    print(training_dir, os.path.join(training_dir, f[0]))\r\n",
      "    df = pd.read_csv(os.path.join(training_dir, f[0]), delimiter=',', error_bad_lines=False)\r\n",
      "    df = df[['user_id', 'product_id', 'event_type', 'category_id', 'category_code', 'brand']]\r\n",
      "    df['event_type_digit'] = df['event_type'].apply(lambda x: 4 if x=='purchase' else 3 if x=='cart' else 2 if x=='remove_from_cart' else 1)\r\n",
      "\r\n",
      "    users = df['user_id'].value_counts()\r\n",
      "    products = df['product_id'].value_counts()\r\n",
      "    \r\n",
      "    # Filter long-tail\r\n",
      "    users = users[users >= 5]\r\n",
      "    products = products[products >= 5]\r\n",
      "\r\n",
      "    reduced_df = df.merge(pd.DataFrame({'user_id': users.index})).merge(pd.DataFrame({'product_id': products.index}))\r\n",
      "    users = reduced_df['user_id'].value_counts()\r\n",
      "    products = reduced_df['product_id'].value_counts()\r\n",
      "\r\n",
      "    # Number users and items\r\n",
      "    user_index = pd.DataFrame({'user_id': users.index, 'user': np.arange(users.shape[0])})\r\n",
      "    product_index = pd.DataFrame({'product_id': products.index, 'item': np.arange(products.shape[0])})\r\n",
      "\r\n",
      "    reduced_df = reduced_df.merge(user_index).merge(product_index)\r\n",
      "\r\n",
      "    # Split train and test\r\n",
      "    test_df = reduced_df.groupby('user_id').last().reset_index()\r\n",
      "\r\n",
      "    train_df = reduced_df.merge(test_df[['user_id', 'product_id']], \r\n",
      "                                on=['user_id', 'product_id'], \r\n",
      "                                how='outer', \r\n",
      "                                indicator=True)\r\n",
      "    train_df = train_df[(train_df['_merge'] == 'left_only')]\r\n",
      "\r\n",
      "    # MXNet data iterators\r\n",
      "    train = gluon.data.ArrayDataset(nd.array(train_df['user'].values, dtype=np.float32), \r\n",
      "                                    nd.array(train_df['item'].values, dtype=np.float32),\r\n",
      "                                    nd.array(train_df['event_type_digit'].values, dtype=np.float32))\r\n",
      "    test  = gluon.data.ArrayDataset(nd.array(test_df['user'].values, dtype=np.float32), \r\n",
      "                                    nd.array(test_df['item'].values, dtype=np.float32),\r\n",
      "                                    nd.array(test_df['event_type_digit'].values, dtype=np.float32))\r\n",
      "\r\n",
      "    train_iter = gluon.data.DataLoader(train, shuffle=True, num_workers=4, batch_size=batch_size, last_batch='rollover')\r\n",
      "    test_iter = gluon.data.DataLoader(train, shuffle=True, num_workers=4, batch_size=batch_size, last_batch='rollover')\r\n",
      "\r\n",
      "    return train_iter, test_iter, user_index, product_index \r\n",
      "\r\n",
      "#########\r\n",
      "# Hosting\r\n",
      "#########\r\n",
      "\r\n",
      "def model_fn(model_dir):\r\n",
      "    \"\"\"\r\n",
      "    Load the gluon model. Called once when hosting service starts.\r\n",
      "\r\n",
      "    :param: model_dir The directory where model files are stored.\r\n",
      "    :return: a model (in this case a Gluon network)\r\n",
      "    \"\"\"\r\n",
      "    ctx = mx.cpu()\r\n",
      "    f = open('{}/MFBlock.params'.format(model_dir), 'r')\r\n",
      "    block_params = json.load(f)\r\n",
      "    f.close()\r\n",
      "    net = MFBlock(max_users=block_params['max_users'], \r\n",
      "                  max_items=block_params['max_items'],\r\n",
      "                  num_emb=block_params['num_emb'],\r\n",
      "                  dropout_p=block_params['dropout_p'])\r\n",
      "    net.load_params('{}/model.params'.format(model_dir), ctx)\r\n",
      "    user_index = pd.read_csv('{}/user_index.csv'.format(model_dir))\r\n",
      "    product_index = pd.read_csv('{}/product_index.csv'.format(model_dir))\r\n",
      "    return net, user_index, product_index\r\n",
      "\r\n",
      "\r\n",
      "def transform_fn(net, data, input_content_type, output_content_type):\r\n",
      "    \"\"\"\r\n",
      "    Transform a request using the Gluon model. Called once per request.\r\n",
      "\r\n",
      "    :param net: The Gluon model.\r\n",
      "    :param data: The request payload.\r\n",
      "    :param input_content_type: The request content type.\r\n",
      "    :param output_content_type: The (desired) response content type.\r\n",
      "    :return: response payload and content type.\r\n",
      "    \"\"\"\r\n",
      "    ctx = mx.cpu()\r\n",
      "    #parsed = json.loads(data)\r\n",
      "    #print(data)\r\n",
      "    #print(len(data.split('\\n')))\r\n",
      "\r\n",
      "    f1, f2 = [], []\r\n",
      "    for line in data.split('\\n'):\r\n",
      "        try:\r\n",
      "            #print(line.split(',')[0].strip())\r\n",
      "            f1.append(float(line.split(',')[0].strip()))\r\n",
      "        except:\r\n",
      "            print(\"An exception occurred\")\r\n",
      "\r\n",
      "        try:\r\n",
      "            #print(line.split(',')[1].strip())\r\n",
      "            f2.append(float(line.split(',')[1].strip()))\r\n",
      "        except:\r\n",
      "            print(\"An exception occurred\")\r\n",
      "\r\n",
      "    trained_net, user_index, product_index = net\r\n",
      "    users = pd.DataFrame({'user_id': f1}).merge(user_index, how='left')['user'].values\r\n",
      "    items = pd.DataFrame({'product_id': f2}).merge(product_index, how='left')['item'].values\r\n",
      "    \r\n",
      "    predictions = trained_net(nd.array(users).as_in_context(ctx), nd.array(items).as_in_context(ctx))\r\n",
      "    response_body = json.dumps(predictions.asnumpy().tolist())\r\n",
      "\r\n",
      "    return response_body, output_content_type\r\n"
     ]
    }
   ],
   "source": [
    "!cat recommender.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Locally\n",
    "\n",
    "Now we can test our train function locally.  This helps ensure we don't have any bugs before submitting our code to SageMaker's pre-built MXNet container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# import recommender\n",
    "\n",
    "# local_test_net, local_customer_index, local_product_index = recommender.train(\n",
    "#     {'train': '/tmp/recsys/temp/'}, \n",
    "#     {'num_embeddings': 64, \n",
    "#      'opt': 'sgd', \n",
    "#      'lr': 0.02, \n",
    "#      'momentum': 0.9, \n",
    "#      'wd': 0.,\n",
    "#      'epochs': 3},\n",
    "#     ['local'],\n",
    "#     1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move Data\n",
    "\n",
    "Holding our data in memory works fine when we're interactively exploring a sample of data, but for larger, longer running processes, we'd prefer to run them in the background with SageMaker Training.  To do this, let's move the dataset to S3 so that it can be picked up by SageMaker training.  This is perfect for use cases like periodic re-training, expanding to a larger dataset, or moving production workloads to larger hardware.\n",
    "\n",
    "앞에서 다운로드한 두 데이터 파일을 합쳐서 train용 버킷에 업로드 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cat /tmp/recsys/2019-Oct.csv > /tmp/recsys/2019-Oct-Nov.csv\n",
    "# !tail -n +2 /tmp/recsys/2019-Nov.csv >> /tmp/recsys/2019-Oct-Nov.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boto3.client('s3').upload_file('/tmp/recsys/2019-Oct-Nov.csv', bucket, prefix + '/train/2019-Oct-Nov.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit\n",
    "\n",
    "Now, we can create an MXNet estimator from the SageMaker Python SDK.  To do so, we need to pass in:\n",
    "1. Instance type and count for our SageMaker Training cluster.  SageMaker's MXNet containers support distributed GPU training, so we could easily set this to multiple ml.p2 or ml.p3 instances if we wanted.\n",
    "  - *Note, this would require some changes to our recommender.py script as we would need to setup the context an key value store properly, as well as determining if and how to distribute the training data.*\n",
    "1. An S3 path for out model artifacts and a role with access to S3 input and output paths.\n",
    "1. Hyperparameters for our neural network.  Since with a 64 dimensional embedding, our recommendations reverted too closely to the mean, let's increase this by an order of magnitude when we train outside of our local instance.  We'll also increase the epochs to see how our accuracy evolves over time. We'll leave all other hyperparameters the same.\n",
    "\n",
    "Once we use `.fit()` this creates a SageMaker Training Job that spins up instances, loads the appropriate packages and data, runs our `train` function from `recommender.py`, wraps up and saves model artifacts to S3, and finishes by tearing down the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-06 17:26:29 Starting - Starting the training job...\n",
      "2020-04-06 17:26:31 Starting - Launching requested ML instances...\n",
      "2020-04-06 17:27:27 Starting - Preparing the instances for training......\n",
      "2020-04-06 17:28:29 Downloading - Downloading input data........................\n",
      "2020-04-06 17:32:17 Training - Downloading the training image..\u001b[34m2020-04-06 17:32:38,042 INFO - root - running container entrypoint\u001b[0m\n",
      "\u001b[34m2020-04-06 17:32:38,042 INFO - root - starting train task\u001b[0m\n",
      "\u001b[34m2020-04-06 17:32:38,062 INFO - container_support.training - Training starting\u001b[0m\n",
      "\u001b[34m2020-04-06 17:32:40,730 INFO - mxnet_container.train - MXNetTrainingEnvironment: {'hosts': ['algo-1'], 'user_script_archive': 's3://summit-2020-db-ml/sagemaker-mxnet-2020-04-06-17-26-29-027/source/sourcedir.tar.gz', 'available_gpus': 1, 'output_data_dir': '/opt/ml/output/data/', 'model_dir': '/opt/ml/model', 'sagemaker_region': 'ap-northeast-2', 'channels': {'train': {'S3DistributionType': 'FullyReplicated', 'RecordWrapperType': 'None', 'TrainingInputMode': 'File'}}, '_ps_verbose': 0, 'resource_config': {'current_host': 'algo-1', 'network_interface_name': 'eth0', 'hosts': ['algo-1']}, '_ps_port': 8000, 'job_name': 'sagemaker-mxnet-2020-04-06-17-26-29-027', 'current_host': 'algo-1', '_scheduler_host': 'algo-1', 'hyperparameters': {'wd': 0.0, 'sagemaker_container_log_level': 20, 'momentum': 0.9, 'num_embeddings': 64, 'lr': 0.02, 'sagemaker_region': 'ap-northeast-2', 'sagemaker_job_name': 'sagemaker-mxnet-2020-04-06-17-26-29-027', 'sagemaker_submit_directory': 's3://summit-2020-db-ml/sagemaker-mxnet-2020-04-06-17-26-29-027/source/sourcedir.tar.gz', 'opt': 'sgd', 'epochs': 10, 'sagemaker_program': 'recommender.py', 'sagemaker_enable_cloudwatch_metrics': False}, 'user_script_name': 'recommender.py', 'available_cpus': 8, 'enable_cloudwatch_metrics': False, 'input_config_dir': '/opt/ml/input/config', 'code_dir': '/opt/ml/code', 'input_dir': '/opt/ml/input', '_scheduler_ip': '10.0.73.33', 'container_log_level': 20, 'channel_dirs': {'train': '/opt/ml/input/data/train'}, 'base_dir': '/opt/ml', 'output_dir': '/opt/ml/output', 'user_requirements_file': None}\u001b[0m\n",
      "\u001b[34mDownloading s3://summit-2020-db-ml/sagemaker-mxnet-2020-04-06-17-26-29-027/source/sourcedir.tar.gz to /tmp/script.tar.gz\u001b[0m\n",
      "\u001b[34m2020-04-06 17:32:41,085 INFO - mxnet_container.train - Starting distributed training task\u001b[0m\n",
      "\u001b[34mCollecting pandas\n",
      "  Downloading https://files.pythonhosted.org/packages/74/24/0cdbf8907e1e3bc5a8da03345c23cbed7044330bb8f73bb12e711a640a00/pandas-0.24.2-cp35-cp35m-manylinux1_x86_64.whl (10.0MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.5/dist-packages (from pandas) (2.7.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.5/dist-packages (from pandas) (1.13.3)\u001b[0m\n",
      "\u001b[34mCollecting pytz>=2011k (from pandas)\n",
      "  Downloading https://files.pythonhosted.org/packages/e7/f9/f0b53f88060247251bf481fa6ea62cd0d25bf1b11a87888e53ce5b7c8ad2/pytz-2019.3-py2.py3-none-any.whl (509kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.5/dist-packages (from python-dateutil>=2.5.0->pandas) (1.11.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pytz, pandas\u001b[0m\n",
      "\n",
      "2020-04-06 17:32:37 Training - Training image download completed. Training in progress.\u001b[34mSuccessfully installed pandas-0.24.2 pytz-2019.3\u001b[0m\n",
      "\u001b[34mYou are using pip version 18.0, however version 20.0.2 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.5/dist-packages/mxnet_container/train.py:178: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  train_args = inspect.getargspec(user_module.train)\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train /opt/ml/input/data/train/2019-Oct-Nov.csv\u001b[0m\n",
      "\u001b[34mepoch: 0\u001b[0m\n",
      "\u001b[34mEPOCH 0: MSE ON TRAINING and TEST: 0.2319733785854241. 0.2319730991685365\u001b[0m\n",
      "\u001b[34mepoch: 1\u001b[0m\n",
      "\u001b[34mEPOCH 1: MSE ON TRAINING and TEST: 0.22844027502024283. 0.22844010354682912\u001b[0m\n",
      "\u001b[34mepoch: 2\u001b[0m\n",
      "\u001b[34mEPOCH 2: MSE ON TRAINING and TEST: 0.2282112277204433. 0.22821083499008665\u001b[0m\n",
      "\u001b[34mepoch: 3\u001b[0m\n",
      "\u001b[34mEPOCH 3: MSE ON TRAINING and TEST: 0.2262296048365958. 0.22622967334631117\u001b[0m\n",
      "\u001b[34mepoch: 4\u001b[0m\n",
      "\u001b[34mEPOCH 4: MSE ON TRAINING and TEST: 0.22670263841107. 0.22670276974664053\u001b[0m\n",
      "\u001b[34mepoch: 5\u001b[0m\n",
      "\u001b[34mEPOCH 5: MSE ON TRAINING and TEST: 0.22604956364822312. 0.22604957761674238\u001b[0m\n",
      "\u001b[34mepoch: 6\u001b[0m\n",
      "\u001b[34mEPOCH 6: MSE ON TRAINING and TEST: 0.22550759962051578. 0.22550771822665508\u001b[0m\n",
      "\u001b[34mepoch: 7\u001b[0m\n",
      "\u001b[34mEPOCH 7: MSE ON TRAINING and TEST: 0.2244855446982788. 0.22448543230965032\u001b[0m\n",
      "\u001b[34mepoch: 8\u001b[0m\n",
      "\u001b[34mEPOCH 8: MSE ON TRAINING and TEST: 0.22427075794774978. 0.22427103412004185\u001b[0m\n",
      "\u001b[34mepoch: 9\u001b[0m\n",
      "\u001b[34mEPOCH 9: MSE ON TRAINING and TEST: 0.2230819383444946. 0.22308265880188657\u001b[0m\n",
      "\u001b[34mend of training\u001b[0m\n",
      "\n",
      "2020-04-06 21:59:55 Uploading - Uploading generated training model\n",
      "2020-04-06 22:01:52 Completed - Training job completed\n",
      "Training seconds: 16403\n",
      "Billable seconds: 16403\n"
     ]
    }
   ],
   "source": [
    "opt = 'sgd'\n",
    "lr = 0.02\n",
    "momentum = 0.9\n",
    "wd = 0.\n",
    "\n",
    "m = MXNet('recommender.py', \n",
    "          py_version='py3',\n",
    "          role=role, \n",
    "          train_instance_count=1, \n",
    "          train_instance_type=\"ml.p3.2xlarge\",\n",
    "          output_path='s3://{}/{}/output'.format('summit-2020-db-ml', 'ecommerce-behavior-data'),\n",
    "          hyperparameters={'num_embeddings': 64, \n",
    "                           'opt': opt, \n",
    "                           'lr': lr, \n",
    "                           'momentum': momentum, \n",
    "                           'wd': wd,\n",
    "                           'epochs': 10},\n",
    "         framework_version='1.1')\n",
    "\n",
    "m.fit({'train': 's3://{}/{}/train/'.format('summit-2020-db-ml', 'ecommerce-behavior-data')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Host\n",
    "\n",
    "Now that we've trained our model, deploying it to a real-time, production endpoint is easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------!"
     ]
    }
   ],
   "source": [
    "predictor = m.deploy(initial_instance_count=1, \n",
    "                     instance_type='ml.m4.xlarge')\n",
    "predictor.serializer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an endpoint, let's test it out.  We'll predict user #6's ratings for the top and bottom ASINs from our local model.\n",
    "\n",
    "*This could be done by sending HTTP POST requests from a separate web service, but to keep things easy, we'll just use the `.predict()` method from the SageMaker Python SDK.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8590192794799805]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict('512505687,44600062')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note, some of our predictions are actually greater than 5, which is to be expected as we didn't do anything special to account for ratings being capped at that value.  Since we are only looking to ranking by predicted rating, this won't create problems for our specific use case.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "Let's start by calculating a naive baseline to approximate how well our model is doing.  The simplest estimate would be to assume every user item rating is just the average rating over all ratings.\n",
    "\n",
    "*Note, we could do better by using each individual video's average, however, in this case it doesn't really matter as the same conclusions would hold.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive MSE: 0.14940589462402137\n"
     ]
    }
   ],
   "source": [
    "print('Naive MSE:', np.mean((test_df['event_type_digit'] - np.mean(train_df['event_type_digit'])) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = []\n",
    "for array in np.array_split(test_df[['user_id', 'product_id']].values, 40):\n",
    "    test_str = ''\n",
    "    for i in array:\n",
    "        for j in i:\n",
    "            if j == i[0]:\n",
    "                test_str += str(j) + ','\n",
    "            else :\n",
    "                test_str += str(j)\n",
    "        test_str += '\\n'\n",
    "    test_preds += predictor.predict(test_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll calculate predictions for our test dataset.\n",
    "\n",
    "*Note, this will align closely to our CloudWatch output above, but may differ slightly due to skipping partial mini-batches in our eval_net function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.14525561152558716\n"
     ]
    }
   ],
   "source": [
    "test_preds = np.array(test_preds)\n",
    "print('MSE:', np.mean((test_df['event_type_digit'] - test_preds) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our neural network and embedding model produces substantially better results (~1.27 vs 1.65 on mean square error).\n",
    "\n",
    "For recommender systems, subjective accuracy also matters.  Let's get some recommendations for a random user to see if they make intuitive sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>event_type_digit</th>\n",
       "      <th>title</th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30213710</th>\n",
       "      <td>516308435</td>\n",
       "      <td>28101452</td>\n",
       "      <td>4</td>\n",
       "      <td>2053013564918072245</td>\n",
       "      <td>8</td>\n",
       "      <td>15673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34382684</th>\n",
       "      <td>516308435</td>\n",
       "      <td>28101570</td>\n",
       "      <td>4</td>\n",
       "      <td>kartal</td>\n",
       "      <td>8</td>\n",
       "      <td>18621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33963252</th>\n",
       "      <td>516308435</td>\n",
       "      <td>28101450</td>\n",
       "      <td>4</td>\n",
       "      <td>2053013564918072245</td>\n",
       "      <td>8</td>\n",
       "      <td>23239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30214174</th>\n",
       "      <td>516308435</td>\n",
       "      <td>28101454</td>\n",
       "      <td>4</td>\n",
       "      <td>raveltex</td>\n",
       "      <td>8</td>\n",
       "      <td>30514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1782223</th>\n",
       "      <td>516308435</td>\n",
       "      <td>17300768</td>\n",
       "      <td>1</td>\n",
       "      <td>2053013553853497655</td>\n",
       "      <td>8</td>\n",
       "      <td>624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72089</th>\n",
       "      <td>516308435</td>\n",
       "      <td>49500007</td>\n",
       "      <td>1</td>\n",
       "      <td>2134905041696326587</td>\n",
       "      <td>8</td>\n",
       "      <td>822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72090</th>\n",
       "      <td>516308435</td>\n",
       "      <td>49500007</td>\n",
       "      <td>1</td>\n",
       "      <td>2134905041696326587</td>\n",
       "      <td>8</td>\n",
       "      <td>822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23999809</th>\n",
       "      <td>516308435</td>\n",
       "      <td>3800751</td>\n",
       "      <td>1</td>\n",
       "      <td>2053013566176363511</td>\n",
       "      <td>8</td>\n",
       "      <td>927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17804269</th>\n",
       "      <td>516308435</td>\n",
       "      <td>28715829</td>\n",
       "      <td>1</td>\n",
       "      <td>2116907519078040377</td>\n",
       "      <td>8</td>\n",
       "      <td>959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17804270</th>\n",
       "      <td>516308435</td>\n",
       "      <td>28715829</td>\n",
       "      <td>1</td>\n",
       "      <td>2116907519078040377</td>\n",
       "      <td>8</td>\n",
       "      <td>959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10421856</th>\n",
       "      <td>516308435</td>\n",
       "      <td>28101002</td>\n",
       "      <td>1</td>\n",
       "      <td>2053013564918072245</td>\n",
       "      <td>8</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10421857</th>\n",
       "      <td>516308435</td>\n",
       "      <td>28101002</td>\n",
       "      <td>1</td>\n",
       "      <td>2053013564918072245</td>\n",
       "      <td>8</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10421858</th>\n",
       "      <td>516308435</td>\n",
       "      <td>28101002</td>\n",
       "      <td>1</td>\n",
       "      <td>2053013564918072245</td>\n",
       "      <td>8</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61473</th>\n",
       "      <td>516308435</td>\n",
       "      <td>44500016</td>\n",
       "      <td>1</td>\n",
       "      <td>omabelle</td>\n",
       "      <td>8</td>\n",
       "      <td>1294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4977131</th>\n",
       "      <td>516308435</td>\n",
       "      <td>25800005</td>\n",
       "      <td>1</td>\n",
       "      <td>2053013562292437791</td>\n",
       "      <td>8</td>\n",
       "      <td>1440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12683097</th>\n",
       "      <td>516308435</td>\n",
       "      <td>28716931</td>\n",
       "      <td>1</td>\n",
       "      <td>2053013565639492569</td>\n",
       "      <td>8</td>\n",
       "      <td>1655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13374145</th>\n",
       "      <td>516308435</td>\n",
       "      <td>16300056</td>\n",
       "      <td>1</td>\n",
       "      <td>2053013558131687989</td>\n",
       "      <td>8</td>\n",
       "      <td>1897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11223680</th>\n",
       "      <td>516308435</td>\n",
       "      <td>23700007</td>\n",
       "      <td>1</td>\n",
       "      <td>2053013561847841545</td>\n",
       "      <td>8</td>\n",
       "      <td>2354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11223681</th>\n",
       "      <td>516308435</td>\n",
       "      <td>23700007</td>\n",
       "      <td>1</td>\n",
       "      <td>2053013561847841545</td>\n",
       "      <td>8</td>\n",
       "      <td>2354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11223682</th>\n",
       "      <td>516308435</td>\n",
       "      <td>23700007</td>\n",
       "      <td>1</td>\n",
       "      <td>2053013561847841545</td>\n",
       "      <td>8</td>\n",
       "      <td>2354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11223683</th>\n",
       "      <td>516308435</td>\n",
       "      <td>23700007</td>\n",
       "      <td>1</td>\n",
       "      <td>2053013561847841545</td>\n",
       "      <td>8</td>\n",
       "      <td>2354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11223684</th>\n",
       "      <td>516308435</td>\n",
       "      <td>23700007</td>\n",
       "      <td>1</td>\n",
       "      <td>2053013561847841545</td>\n",
       "      <td>8</td>\n",
       "      <td>2354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11223685</th>\n",
       "      <td>516308435</td>\n",
       "      <td>23700007</td>\n",
       "      <td>1</td>\n",
       "      <td>2053013561847841545</td>\n",
       "      <td>8</td>\n",
       "      <td>2354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11223686</th>\n",
       "      <td>516308435</td>\n",
       "      <td>23700007</td>\n",
       "      <td>1</td>\n",
       "      <td>2053013561847841545</td>\n",
       "      <td>8</td>\n",
       "      <td>2354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11223687</th>\n",
       "      <td>516308435</td>\n",
       "      <td>23700007</td>\n",
       "      <td>1</td>\n",
       "      <td>2053013561847841545</td>\n",
       "      <td>8</td>\n",
       "      <td>2354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11223688</th>\n",
       "      <td>516308435</td>\n",
       "      <td>23700007</td>\n",
       "      <td>1</td>\n",
       "      <td>2053013561847841545</td>\n",
       "      <td>8</td>\n",
       "      <td>2354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16290229</th>\n",
       "      <td>516308435</td>\n",
       "      <td>28100989</td>\n",
       "      <td>1</td>\n",
       "      <td>2053013564918072245</td>\n",
       "      <td>8</td>\n",
       "      <td>2364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16290230</th>\n",
       "      <td>516308435</td>\n",
       "      <td>28100989</td>\n",
       "      <td>1</td>\n",
       "      <td>2053013564918072245</td>\n",
       "      <td>8</td>\n",
       "      <td>2364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25865180</th>\n",
       "      <td>516308435</td>\n",
       "      <td>28100913</td>\n",
       "      <td>1</td>\n",
       "      <td>2053013564918072245</td>\n",
       "      <td>8</td>\n",
       "      <td>2604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25865181</th>\n",
       "      <td>516308435</td>\n",
       "      <td>28100913</td>\n",
       "      <td>1</td>\n",
       "      <td>2053013564918072245</td>\n",
       "      <td>8</td>\n",
       "      <td>2604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37710674</th>\n",
       "      <td>516308435</td>\n",
       "      <td>28102781</td>\n",
       "      <td>1</td>\n",
       "      <td>baltekstil</td>\n",
       "      <td>8</td>\n",
       "      <td>98691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37710675</th>\n",
       "      <td>516308435</td>\n",
       "      <td>28102781</td>\n",
       "      <td>1</td>\n",
       "      <td>baltekstil</td>\n",
       "      <td>8</td>\n",
       "      <td>98691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37710575</th>\n",
       "      <td>516308435</td>\n",
       "      <td>28102708</td>\n",
       "      <td>1</td>\n",
       "      <td>kartal</td>\n",
       "      <td>8</td>\n",
       "      <td>99270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37710576</th>\n",
       "      <td>516308435</td>\n",
       "      <td>28102708</td>\n",
       "      <td>1</td>\n",
       "      <td>kartal</td>\n",
       "      <td>8</td>\n",
       "      <td>99270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37710577</th>\n",
       "      <td>516308435</td>\n",
       "      <td>28102708</td>\n",
       "      <td>1</td>\n",
       "      <td>kartal</td>\n",
       "      <td>8</td>\n",
       "      <td>99270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37710578</th>\n",
       "      <td>516308435</td>\n",
       "      <td>28102708</td>\n",
       "      <td>1</td>\n",
       "      <td>kartal</td>\n",
       "      <td>8</td>\n",
       "      <td>99270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36145197</th>\n",
       "      <td>516308435</td>\n",
       "      <td>28101152</td>\n",
       "      <td>1</td>\n",
       "      <td>2053013564918072245</td>\n",
       "      <td>8</td>\n",
       "      <td>101379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36145198</th>\n",
       "      <td>516308435</td>\n",
       "      <td>28101152</td>\n",
       "      <td>1</td>\n",
       "      <td>2053013564918072245</td>\n",
       "      <td>8</td>\n",
       "      <td>101379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36145199</th>\n",
       "      <td>516308435</td>\n",
       "      <td>28101152</td>\n",
       "      <td>1</td>\n",
       "      <td>2053013564918072245</td>\n",
       "      <td>8</td>\n",
       "      <td>101379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19292760</th>\n",
       "      <td>516308435</td>\n",
       "      <td>28101122</td>\n",
       "      <td>1</td>\n",
       "      <td>2053013564918072245</td>\n",
       "      <td>8</td>\n",
       "      <td>101486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37710767</th>\n",
       "      <td>516308435</td>\n",
       "      <td>28102339</td>\n",
       "      <td>1</td>\n",
       "      <td>shiraz</td>\n",
       "      <td>8</td>\n",
       "      <td>102243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37710768</th>\n",
       "      <td>516308435</td>\n",
       "      <td>28102339</td>\n",
       "      <td>1</td>\n",
       "      <td>shiraz</td>\n",
       "      <td>8</td>\n",
       "      <td>102243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37710769</th>\n",
       "      <td>516308435</td>\n",
       "      <td>28102339</td>\n",
       "      <td>1</td>\n",
       "      <td>shiraz</td>\n",
       "      <td>8</td>\n",
       "      <td>102243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37710007</th>\n",
       "      <td>516308435</td>\n",
       "      <td>28100770</td>\n",
       "      <td>1</td>\n",
       "      <td>2053013564918072245</td>\n",
       "      <td>8</td>\n",
       "      <td>102918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37710008</th>\n",
       "      <td>516308435</td>\n",
       "      <td>28100770</td>\n",
       "      <td>1</td>\n",
       "      <td>2053013564918072245</td>\n",
       "      <td>8</td>\n",
       "      <td>102918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33292587</th>\n",
       "      <td>516308435</td>\n",
       "      <td>28102746</td>\n",
       "      <td>1</td>\n",
       "      <td>kartal</td>\n",
       "      <td>8</td>\n",
       "      <td>103852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19940071</th>\n",
       "      <td>516308435</td>\n",
       "      <td>28101988</td>\n",
       "      <td>1</td>\n",
       "      <td>almada</td>\n",
       "      <td>8</td>\n",
       "      <td>104219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19292505</th>\n",
       "      <td>516308435</td>\n",
       "      <td>28101254</td>\n",
       "      <td>1</td>\n",
       "      <td>kartal</td>\n",
       "      <td>8</td>\n",
       "      <td>104314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36456252</th>\n",
       "      <td>516308435</td>\n",
       "      <td>28102394</td>\n",
       "      <td>1</td>\n",
       "      <td>2053013564918072245</td>\n",
       "      <td>8</td>\n",
       "      <td>105456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36519556</th>\n",
       "      <td>516308435</td>\n",
       "      <td>28100737</td>\n",
       "      <td>1</td>\n",
       "      <td>2053013564918072245</td>\n",
       "      <td>8</td>\n",
       "      <td>105486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37710755</th>\n",
       "      <td>516308435</td>\n",
       "      <td>28102736</td>\n",
       "      <td>1</td>\n",
       "      <td>eviza</td>\n",
       "      <td>8</td>\n",
       "      <td>106312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37710756</th>\n",
       "      <td>516308435</td>\n",
       "      <td>28102736</td>\n",
       "      <td>1</td>\n",
       "      <td>eviza</td>\n",
       "      <td>8</td>\n",
       "      <td>106312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37710801</th>\n",
       "      <td>516308435</td>\n",
       "      <td>28101154</td>\n",
       "      <td>1</td>\n",
       "      <td>2053013564918072245</td>\n",
       "      <td>8</td>\n",
       "      <td>106619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36519587</th>\n",
       "      <td>516308435</td>\n",
       "      <td>28100736</td>\n",
       "      <td>1</td>\n",
       "      <td>2053013564918072245</td>\n",
       "      <td>8</td>\n",
       "      <td>106779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37125513</th>\n",
       "      <td>516308435</td>\n",
       "      <td>28101995</td>\n",
       "      <td>1</td>\n",
       "      <td>almada</td>\n",
       "      <td>8</td>\n",
       "      <td>107447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37125514</th>\n",
       "      <td>516308435</td>\n",
       "      <td>28101995</td>\n",
       "      <td>1</td>\n",
       "      <td>almada</td>\n",
       "      <td>8</td>\n",
       "      <td>107447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36145210</th>\n",
       "      <td>516308435</td>\n",
       "      <td>28100731</td>\n",
       "      <td>1</td>\n",
       "      <td>2053013564918072245</td>\n",
       "      <td>8</td>\n",
       "      <td>107923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36519546</th>\n",
       "      <td>516308435</td>\n",
       "      <td>28101201</td>\n",
       "      <td>1</td>\n",
       "      <td>kartal</td>\n",
       "      <td>8</td>\n",
       "      <td>114245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36519547</th>\n",
       "      <td>516308435</td>\n",
       "      <td>28101201</td>\n",
       "      <td>1</td>\n",
       "      <td>kartal</td>\n",
       "      <td>8</td>\n",
       "      <td>114245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37710813</th>\n",
       "      <td>516308435</td>\n",
       "      <td>28102772</td>\n",
       "      <td>1</td>\n",
       "      <td>baltekstil</td>\n",
       "      <td>8</td>\n",
       "      <td>114914</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2311 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            user_id  product_id  event_type_digit                title  user  \\\n",
       "30213710  516308435    28101452                 4  2053013564918072245     8   \n",
       "34382684  516308435    28101570                 4               kartal     8   \n",
       "33963252  516308435    28101450                 4  2053013564918072245     8   \n",
       "30214174  516308435    28101454                 4             raveltex     8   \n",
       "1782223   516308435    17300768                 1  2053013553853497655     8   \n",
       "72089     516308435    49500007                 1  2134905041696326587     8   \n",
       "72090     516308435    49500007                 1  2134905041696326587     8   \n",
       "23999809  516308435     3800751                 1  2053013566176363511     8   \n",
       "17804269  516308435    28715829                 1  2116907519078040377     8   \n",
       "17804270  516308435    28715829                 1  2116907519078040377     8   \n",
       "10421856  516308435    28101002                 1  2053013564918072245     8   \n",
       "10421857  516308435    28101002                 1  2053013564918072245     8   \n",
       "10421858  516308435    28101002                 1  2053013564918072245     8   \n",
       "61473     516308435    44500016                 1             omabelle     8   \n",
       "4977131   516308435    25800005                 1  2053013562292437791     8   \n",
       "12683097  516308435    28716931                 1  2053013565639492569     8   \n",
       "13374145  516308435    16300056                 1  2053013558131687989     8   \n",
       "11223680  516308435    23700007                 1  2053013561847841545     8   \n",
       "11223681  516308435    23700007                 1  2053013561847841545     8   \n",
       "11223682  516308435    23700007                 1  2053013561847841545     8   \n",
       "11223683  516308435    23700007                 1  2053013561847841545     8   \n",
       "11223684  516308435    23700007                 1  2053013561847841545     8   \n",
       "11223685  516308435    23700007                 1  2053013561847841545     8   \n",
       "11223686  516308435    23700007                 1  2053013561847841545     8   \n",
       "11223687  516308435    23700007                 1  2053013561847841545     8   \n",
       "11223688  516308435    23700007                 1  2053013561847841545     8   \n",
       "16290229  516308435    28100989                 1  2053013564918072245     8   \n",
       "16290230  516308435    28100989                 1  2053013564918072245     8   \n",
       "25865180  516308435    28100913                 1  2053013564918072245     8   \n",
       "25865181  516308435    28100913                 1  2053013564918072245     8   \n",
       "...             ...         ...               ...                  ...   ...   \n",
       "37710674  516308435    28102781                 1           baltekstil     8   \n",
       "37710675  516308435    28102781                 1           baltekstil     8   \n",
       "37710575  516308435    28102708                 1               kartal     8   \n",
       "37710576  516308435    28102708                 1               kartal     8   \n",
       "37710577  516308435    28102708                 1               kartal     8   \n",
       "37710578  516308435    28102708                 1               kartal     8   \n",
       "36145197  516308435    28101152                 1  2053013564918072245     8   \n",
       "36145198  516308435    28101152                 1  2053013564918072245     8   \n",
       "36145199  516308435    28101152                 1  2053013564918072245     8   \n",
       "19292760  516308435    28101122                 1  2053013564918072245     8   \n",
       "37710767  516308435    28102339                 1               shiraz     8   \n",
       "37710768  516308435    28102339                 1               shiraz     8   \n",
       "37710769  516308435    28102339                 1               shiraz     8   \n",
       "37710007  516308435    28100770                 1  2053013564918072245     8   \n",
       "37710008  516308435    28100770                 1  2053013564918072245     8   \n",
       "33292587  516308435    28102746                 1               kartal     8   \n",
       "19940071  516308435    28101988                 1               almada     8   \n",
       "19292505  516308435    28101254                 1               kartal     8   \n",
       "36456252  516308435    28102394                 1  2053013564918072245     8   \n",
       "36519556  516308435    28100737                 1  2053013564918072245     8   \n",
       "37710755  516308435    28102736                 1                eviza     8   \n",
       "37710756  516308435    28102736                 1                eviza     8   \n",
       "37710801  516308435    28101154                 1  2053013564918072245     8   \n",
       "36519587  516308435    28100736                 1  2053013564918072245     8   \n",
       "37125513  516308435    28101995                 1               almada     8   \n",
       "37125514  516308435    28101995                 1               almada     8   \n",
       "36145210  516308435    28100731                 1  2053013564918072245     8   \n",
       "36519546  516308435    28101201                 1               kartal     8   \n",
       "36519547  516308435    28101201                 1               kartal     8   \n",
       "37710813  516308435    28102772                 1           baltekstil     8   \n",
       "\n",
       "            item  \n",
       "30213710   15673  \n",
       "34382684   18621  \n",
       "33963252   23239  \n",
       "30214174   30514  \n",
       "1782223      624  \n",
       "72089        822  \n",
       "72090        822  \n",
       "23999809     927  \n",
       "17804269     959  \n",
       "17804270     959  \n",
       "10421856    1000  \n",
       "10421857    1000  \n",
       "10421858    1000  \n",
       "61473       1294  \n",
       "4977131     1440  \n",
       "12683097    1655  \n",
       "13374145    1897  \n",
       "11223680    2354  \n",
       "11223681    2354  \n",
       "11223682    2354  \n",
       "11223683    2354  \n",
       "11223684    2354  \n",
       "11223685    2354  \n",
       "11223686    2354  \n",
       "11223687    2354  \n",
       "11223688    2354  \n",
       "16290229    2364  \n",
       "16290230    2364  \n",
       "25865180    2604  \n",
       "25865181    2604  \n",
       "...          ...  \n",
       "37710674   98691  \n",
       "37710675   98691  \n",
       "37710575   99270  \n",
       "37710576   99270  \n",
       "37710577   99270  \n",
       "37710578   99270  \n",
       "36145197  101379  \n",
       "36145198  101379  \n",
       "36145199  101379  \n",
       "19292760  101486  \n",
       "37710767  102243  \n",
       "37710768  102243  \n",
       "37710769  102243  \n",
       "37710007  102918  \n",
       "37710008  102918  \n",
       "33292587  103852  \n",
       "19940071  104219  \n",
       "19292505  104314  \n",
       "36456252  105456  \n",
       "36519556  105486  \n",
       "37710755  106312  \n",
       "37710756  106312  \n",
       "37710801  106619  \n",
       "36519587  106779  \n",
       "37125513  107447  \n",
       "37125514  107447  \n",
       "36145210  107923  \n",
       "36519546  114245  \n",
       "36519547  114245  \n",
       "37710813  114914  \n",
       "\n",
       "[2311 rows x 6 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_df[reduced_df['user'] == 8].sort_values(['event_type_digit', 'item'], ascending=[False, True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Wrap-up\n",
    "\n",
    "In this example, we developed a deep learning model to predict customer purchase.  This could serve as the foundation of a recommender system in a variety of use cases.  However, there are many ways in which it could be improved.  For example we did very little with:\n",
    "- hyperparameter tuning\n",
    "- controlling for overfitting (early stopping, dropout, etc.)\n",
    "- testing whether binarizing our target variable would improve results\n",
    "- including other information sources (video genres, historical ratings, time of review)\n",
    "- adjusting our threshold for user and item inclusion \n",
    "\n",
    "In addition to improving the model, we could improve the engineering by:\n",
    "- Setting the context and key value store up for distributed training\n",
    "- Fine tuning our data ingestion (e.g. num_workers on our data iterators) to ensure we're fully utilizing our GPU\n",
    "- Thinking about how pre-processing would need to change as datasets scale beyond a single machine\n",
    "\n",
    "Beyond that, recommenders are a very active area of research and techniques from active learning, reinforcement learning, segmentation, ensembling, and more should be investigated to deliver well-rounded recommendations.\n",
    "\n",
    "### Clean-up (optional)\n",
    "\n",
    "Let's finish by deleting our endpoint to avoid stray hosting charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sagemaker.Session().delete_endpoint(predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
